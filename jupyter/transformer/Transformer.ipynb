{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7a2599",
   "metadata": {},
   "source": [
    "# Transformer究竟是怎么回事！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21af3869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 4])\n",
      "多头自注意力(dropout) tensor([[[[0.3432, 0.3573, 0.1792, 0.2315],\n",
      "          [0.1828, 0.0000, 0.0000, 0.3092],\n",
      "          [0.3669, 0.3876, 0.0000, 0.2105],\n",
      "          [0.3692, 0.4144, 0.1382, 0.1893]],\n",
      "\n",
      "         [[0.2567, 0.2479, 0.2985, 0.0000],\n",
      "          [0.2744, 0.2937, 0.2774, 0.2657],\n",
      "          [0.3706, 0.1436, 0.2149, 0.3820],\n",
      "          [0.2568, 0.1715, 0.0000, 0.3868]]],\n",
      "\n",
      "\n",
      "        [[[0.2671, 0.3445, 0.2570, 0.2425],\n",
      "          [0.2731, 0.0000, 0.3076, 0.2894],\n",
      "          [0.2793, 0.3135, 0.2533, 0.2650],\n",
      "          [0.2226, 0.0811, 0.4601, 0.3473]],\n",
      "\n",
      "         [[0.3057, 0.2948, 0.2297, 0.2809],\n",
      "          [0.1975, 0.1582, 0.1898, 0.5655],\n",
      "          [0.2736, 0.2691, 0.2725, 0.2959],\n",
      "          [0.2823, 0.2601, 0.0000, 0.3430]]]], grad_fn=<MulBackward0>)\n",
      "MultiHeadAttentionLayer output before concat tensor([[[[-0.0426,  1.0532],\n",
      "          [-0.1813,  0.8385],\n",
      "          [-0.0677,  0.7008],\n",
      "          [-0.0256,  0.8951]],\n",
      "\n",
      "         [[-0.0383, -0.3361],\n",
      "          [ 0.0954, -0.6275],\n",
      "          [ 0.3232, -0.7576],\n",
      "          [ 0.2985, -0.6120]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0345,  1.3356],\n",
      "          [-0.2022,  0.8932],\n",
      "          [-0.0062,  1.3363],\n",
      "          [-0.2381,  1.0232]],\n",
      "\n",
      "         [[ 0.1024, -0.6527],\n",
      "          [ 0.6758, -0.7771],\n",
      "          [ 0.1482, -0.6484],\n",
      "          [ 0.2585, -0.6053]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "MultiHeadAttentionLayer output after concat tensor([[[-0.0426,  1.0532, -0.0383, -0.3361],\n",
      "         [-0.1813,  0.8385,  0.0954, -0.6275],\n",
      "         [-0.0677,  0.7008,  0.3232, -0.7576],\n",
      "         [-0.0256,  0.8951,  0.2985, -0.6120]],\n",
      "\n",
      "        [[ 0.0345,  1.3356,  0.1024, -0.6527],\n",
      "         [-0.2022,  0.8932,  0.6758, -0.7771],\n",
      "         [-0.0062,  1.3363,  0.1482, -0.6484],\n",
      "         [-0.2381,  1.0232,  0.2585, -0.6053]]], grad_fn=<ViewBackward0>)\n",
      "单头自注意力(dropout) tensor([[[0.3114, 0.3127, 0.2188, 0.2682],\n",
      "         [0.2087, 0.0000, 0.0000, 0.2959],\n",
      "         [0.4314, 0.2293, 0.0000, 0.2975],\n",
      "         [0.3472, 0.2831, 0.1916, 0.2892]],\n",
      "\n",
      "        [[0.2893, 0.3375, 0.2300, 0.0000],\n",
      "         [0.2208, 0.1728, 0.2335, 0.4840],\n",
      "         [0.2763, 0.2963, 0.2571, 0.2814],\n",
      "         [0.2491, 0.1152, 0.0000, 0.3915]]], grad_fn=<MulBackward0>)\n",
      "SingleHeadAttentionLayer output tensor([[[-0.0492,  1.1921,  0.1329, -0.6368],\n",
      "         [-0.1924,  0.8233,  0.3049, -0.4483],\n",
      "         [-0.2041,  0.9528,  0.3492, -0.6471],\n",
      "         [-0.0940,  1.2169,  0.1938, -0.6660]],\n",
      "\n",
      "        [[ 0.3542,  1.0782, -0.3560, -0.3947],\n",
      "         [-0.3988,  1.2687,  0.5275, -0.7397],\n",
      "         [-0.0377,  1.3248,  0.1075, -0.6392],\n",
      "         [-0.3270,  1.0767,  0.4469, -0.5856]]], grad_fn=<UnsafeViewBackward0>)\n",
      "W_q 权重是否相同: True\n",
      "W_k 权重是否相同: True\n",
      "W_v 权重是否相同: True\n",
      "W_o 权重是否相同: True\n",
      "多头与单头output差异（绝对值）: [[[0.15315025 0.02567017 0.04456654 0.13397968]\n",
      "  [0.02817568 0.13419235 0.13455245 0.01605618]\n",
      "  [0.00038713 0.13090661 0.08600353 0.04794151]\n",
      "  [0.04133165 0.03679121 0.04183593 0.03207263]]\n",
      "\n",
      " [[0.21144041 0.24567547 0.08537814 0.32857895]\n",
      "  [0.01147264 0.08135319 0.01028851 0.04924464]\n",
      "  [0.01085898 0.01143956 0.03564259 0.00235754]\n",
      "  [0.03496635 0.11395574 0.07011586 0.08406445]]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机数种子\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 位置编码模块\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 多头自注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) # [batch_size, num_heads, seq_len, d_k]\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) # [batch_size, num_heads, seq_len, d_k]\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) # [batch_size, num_heads, seq_len, d_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = torch.softmax(scores, dim=-1) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(\"多头自注意力(dropout)\",attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V) # [batch_size, num_heads, seq_len, d_k]\n",
    "        print(\"MultiHeadAttentionLayer output before concat\",attn_output)\n",
    "        # [batch_size, seq_len, d_model] 包含着 num_heads 个含有上下文信息的向量\n",
    "        # 原本一个token对应一个注意力，现在一个token对应 num_heads 个注意力\n",
    "        # 也就是说一个token可能会对应多个不同的上下文信息，比如语义，语法，逻辑等\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        print(\"MultiHeadAttentionLayer output after concat\",attn_output)\n",
    "        self.output = self.W_o(attn_output)\n",
    "        return self.output\n",
    "\n",
    "# 单头注意力模块\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.W_q(x) # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x) # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(x) # [batch_size, seq_len, d_model]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model) # [batch_size, seq_len, seq_len]\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(\"单头自注意力(dropout)\",attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V) # [batch_size, seq_len, d_model]\n",
    "        print(\"SingleHeadAttentionLayer output\",attn_output)\n",
    "        self.output = self.W_o(attn_output)\n",
    "        return self.output\n",
    "\n",
    "# 前馈网络\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Transformer层（多头）\n",
    "class MultiHeadTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(MultiHeadTransformerLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "# Transformer层（单头）\n",
    "class SingleHeadTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(SingleHeadTransformerLayer, self).__init__()\n",
    "        self.attention = SingleHeadAttention(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "# 简单Transformer模型\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, max_len=5000, use_multi_head=True):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.use_multi_head = use_multi_head\n",
    "        if use_multi_head:\n",
    "            self.transformer_layer = MultiHeadTransformerLayer(d_model, num_heads, d_ff)\n",
    "        else:\n",
    "            self.transformer_layer = SingleHeadTransformerLayer(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.pos_encoding(embedded)\n",
    "        output = self.transformer_layer(embedded)\n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 固定随机数种子\n",
    "    set_seed(42)\n",
    "\n",
    "    vocab_size = 100\n",
    "    d_model = 4\n",
    "    num_heads = 2\n",
    "    d_ff = 4 * d_model\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "\n",
    "    # 输入数据\n",
    "    x = torch.tensor([[1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "    print(\"输入形状:\", x.shape)\n",
    "\n",
    "    # 多头注意力模型\n",
    "    set_seed(42)  # 确保每次初始化前种子一致\n",
    "    multi_head_model = SimpleTransformer(vocab_size, d_model, num_heads, d_ff, use_multi_head=True)\n",
    "    multi_head_output = multi_head_model(x) # [batch_size, seq_len, d_model]\n",
    "    # print(\"多头注意力输出:\", multi_head_output.detach().numpy())\n",
    "\n",
    "    # 单头注意力模型\n",
    "    set_seed(42)  # 再次设置相同种子\n",
    "    single_head_model = SimpleTransformer(vocab_size, d_model, num_heads, d_ff, use_multi_head=False)\n",
    "    single_head_output = single_head_model(x) # [batch_size, seq_len, d_model]\n",
    "    # print(\"单头注意力输出:\", single_head_output.detach().numpy())\n",
    "\n",
    "    # 验证权重是否相同\n",
    "    print(\"W_q 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_q.weight, single_head_model.transformer_layer.attention.W_q.weight))\n",
    "    print(\"W_k 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_k.weight, single_head_model.transformer_layer.attention.W_k.weight))\n",
    "    print(\"W_v 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_v.weight, single_head_model.transformer_layer.attention.W_v.weight))\n",
    "    print(\"W_o 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_o.weight, single_head_model.transformer_layer.attention.W_o.weight))\n",
    "\n",
    "    # 计算输出差异\n",
    "    diff = torch.abs(multi_head_model.transformer_layer.attention.output - single_head_model.transformer_layer.attention.output)\n",
    "    print(\"多头与单头output差异（绝对值）:\", diff.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c89b4c6d6dd5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T07:34:12.730811Z",
     "start_time": "2025-04-04T07:34:09.048524Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "# 位置编码模块\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建位置编码表，形状为 [max_len, d_model]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 生成位置索引 [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算频率项，用于正弦和余弦函数\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 偶数维度用正弦函数\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 奇数维度用余弦函数\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 增加 batch 维度，形状变为 [1, max_len, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 注册为缓冲区，不参与梯度更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # 截取与输入序列长度匹配的位置编码，并加到输入上\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 多头自注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0  # 确保 d_model 可以被 num_heads 整除\n",
    "        self.d_model = d_model  # 模型维度\n",
    "        self.num_heads = num_heads  # 注意力头数\n",
    "        self.d_k = d_model // num_heads  # 每个头的维度\n",
    "\n",
    "        # 定义 Q、K、V 的线性变换层\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # 查询矩阵\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # 键矩阵\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # 值矩阵\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # 输出投影矩阵\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "\n",
    "        # 1. 计算 Q、K、V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # torch常用变换形状的函数\n",
    "        # view 将张量重新调整为指定的形状，要求新形状的元素总数与原张量一致\n",
    "        # reshape 类似 view，但更灵活，要求元素总数不变\n",
    "        # transpose 交换张量的两个维度，不改变数据顺序，只能交换两个维度\n",
    "        # unsqueeze 在指定位置增加一个维度（大小为 1），用于扩充张量维度\n",
    "        # squeeze 移除张量中大小为 1 的维度，用于缩小维度\n",
    "        # permute 重新排列张量的所有维度，比 transpose 更灵活。\n",
    "        # expand 将张量在大小为 1 的维度上扩展到指定大小，不复制数据（类似广播）\n",
    "        # 2. 将 Q、K、V 分成多头\n",
    "        # 变换形状为 [batch_size, seq_len, num_heads, d_k]，然后转置为 [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. 计算注意力分数\n",
    "        # scores = Q * K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        # 应用 softmax 得到注意力权重\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # 4. 用注意力权重加权 V\n",
    "        attn_output = torch.matmul(attn_weights, V)  # [batch_size, num_heads, seq_len, d_k]\n",
    "\n",
    "        # 5. 合并多头\n",
    "        # 转置回 [batch_size, seq_len, num_heads, d_k]，然后重塑为 [batch_size, seq_len, d_model]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        # 通过输出投影层\n",
    "        output = self.W_o(attn_output)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        return output\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        # 单头自注意力，Q、K、V 线性变换\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        # 1. 计算 Q、K、V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
    "        # 2. 计算注意力分数\n",
    "        # scores = Q * K^T / sqrt(d_model)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)  # [batch_size, seq_len, seq_len]\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "        # 3. 用注意力权重加权 V\n",
    "        attn_output = torch.matmul(attn_weights, V)  # [batch_size, seq_len, d_model]\n",
    "        # 4. 输出投影\n",
    "        output = self.W_o(attn_output)  # [batch_size, seq_len, d_model]\n",
    "        return output\n",
    "    \n",
    "# 前馈网络模块\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 第一层：扩展到 d_ff 维度\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        # 第二层：压缩回 d_model 维度\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # 通过第一层线性变换和激活函数\n",
    "        x = self.relu(self.linear1(x))  # [batch_size, seq_len, d_ff]\n",
    "        # 通过第二层线性变换\n",
    "        x = self.linear2(x)  # [batch_size, seq_len, d_model]\n",
    "        return x\n",
    "\n",
    "# 单层 Transformer 模块\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        # 多头自注意力\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        # 前馈网络\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # 1. 多头自注意力 + 残差连接 + 归一化\n",
    "        attn_output = self.attention(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.norm1(x + attn_output)  # 残差连接后归一化\n",
    "\n",
    "        # 2. 前馈网络 + 残差连接 + 归一化\n",
    "        ffn_output = self.ffn(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.norm2(x + ffn_output)  # 残差连接后归一化\n",
    "\n",
    "        return x\n",
    "\n",
    "# 完整 Transformer 模型\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, max_len=5000):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        # 嵌入层：将 token 索引转换为 d_model 维向量\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # 位置编码\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        # 单层 Transformer\n",
    "        self.transformer_layer = TransformerLayer(d_model, num_heads, d_ff)\n",
    "        # 输出层：将 d_model 维向量映射到词汇表大小\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]，输入是 token 索引\n",
    "        # 1. 嵌入层：将 token 索引转换为嵌入向量\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 2. 添加位置编码\n",
    "        embedded = self.pos_encoding(embedded)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 3. 通过 Transformer 层\n",
    "        transformed = self.transformer_layer(embedded)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 4. 输出层：映射到词汇表大小\n",
    "        output = self.output_layer(transformed)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return output\n",
    "    \n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 参数设置\n",
    "    vocab_size = 1000  # 词汇表大小\n",
    "    d_model = 4       # 嵌入维度\n",
    "    num_heads = 2      # 注意力头数\n",
    "    d_ff = 16         # 前馈网络中间维度\n",
    "    batch_size = 2     # 批量大小\n",
    "    seq_len = 10       # 序列长度\n",
    "\n",
    "    # 创建模型\n",
    "    model = SimpleTransformer(vocab_size, d_model, num_heads, d_ff)\n",
    "\n",
    "    # 随机输入：模拟 token 索引\n",
    "    x = torch.tensor([[1,2,3,4], [4,3,2,1]])\n",
    "    print(\"输入形状:\", x.shape)\n",
    "\n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    print(\"输出形状:\", output.shape)  # [2, 10, 1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
