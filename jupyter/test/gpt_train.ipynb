{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af00217",
   "metadata": {},
   "source": [
    "# 彻底搞懂GPT是如何训练的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4d5d4",
   "metadata": {},
   "source": [
    "### 1. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113ef48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小: 217561\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "dataset_path = \"/raid/gfc/llm/datasets/ChinesePoems/poems.txt\"\n",
    "# 制作 Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = f.readlines()\n",
    "        self.data = [line.strip() for line in self.data]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 加载数据集\n",
    "dataset = MyDataset(dataset_path)\n",
    "print(f\"数据集大小: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1501bd",
   "metadata": {},
   "source": [
    "### 2. 加载模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a23ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/gfc/anaconda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='uer/gpt2-distil-chinese-cluecorpussmall', vocab_size=21128, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "--------------------------------------------------------------------------------\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(21128, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\", cache_dir=cache_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\", cache_dir=cache_dir).to(device)\n",
    "\n",
    "print(tokenizer)\n",
    "print('--' * 40)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8411084",
   "metadata": {},
   "source": [
    "### 3. 定义dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df2143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13597\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    data = tokenizer.batch_encode_plus(batch, \n",
    "                                       padding=True, \n",
    "                                       truncation=True, \n",
    "                                       max_length=512, \n",
    "                                       return_tensors=\"pt\")\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "    return data\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, \n",
    "    batch_size=16, shuffle=True, \n",
    "    drop_last=True, \n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3036e1",
   "metadata": {},
   "source": [
    "### 4. 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb0f2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Epochs = 1\n",
    "\n",
    "# 定义训练函数\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Epochs)\n",
    "    # 梯度裁剪\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # 1. 对于GPT这类自回归（Causal Language Model, CLM）模型，labels和input_ids是一样的\n",
    "            input_ids = batch['input_ids'].to(device) # [batch_size, seq_len]\n",
    "            labels = batch['labels'].to(device)\n",
    "            # 2. outputs包含loss,logits和past_key_values的字典\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            # print(outputs.logits.shape) # [batch_size, seq_len, vocab_size]\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # 3. 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "              \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    # 1. 切换到评估模式（关闭dropout等）\n",
    "                    model.eval()\n",
    "                    # 2. 取模型输出的预测，每个位置选概率最大的token\n",
    "                    #    去掉最后一个token，因为最后一个token不需要再做预测\n",
    "                    out = outputs.logits.argmax(dim=2)[:, :-1] # [batch_size, seq_len-1]\n",
    "\n",
    "                    # 3. 取labels去掉第一个token，因为第一个token不会成为被预测的目标\n",
    "                    labels = batch['labels'][:, 1:].to(device) # [batch_size, seq_len-1]\n",
    "\n",
    "                    # 4. 只保留非padding部分（labels!=0），避免padding影响准确率\n",
    "                    select = labels != 0\n",
    "                    out = out[select]\n",
    "                    labels = labels[select]\n",
    "\n",
    "                    # 5. 计算准确率（预测正确的token数 / 有效token总数）\n",
    "                    acc = (labels == out).sum().item() / labels.numel()\n",
    "                    # 6. 获取当前学习率，打印日志\n",
    "                    lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch}, Step {i}, Lr {lr:.5e}, Loss {loss:.5f}, Acc {acc:.2%}\")\n",
    "\n",
    "                    # 7. 切回训练模式\n",
    "                    model.train()\n",
    "                    # 8. 删除变量释放显存\n",
    "                    del select\n",
    "                    \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # 每个epoch结束后的操作\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.5f}\")\n",
    "\n",
    "        # 调整学习率\n",
    "        scheduler.step()   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7dc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Lr 2.00000e-05, Loss 9.62540, Acc 13.86%\n",
      "Epoch 0, Step 100, Lr 2.00000e-05, Loss 3.33211, Acc 17.90%\n",
      "Epoch 0, Step 200, Lr 2.00000e-05, Loss 2.25275, Acc 19.11%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9fb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
