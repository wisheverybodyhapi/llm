{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全量微调GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/gfc/anaconda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集结构：\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['dynasty', 'author', 'title', 'content'],\n",
      "        num_rows: 217561\n",
      "    })\n",
      "})\n",
      "\n",
      "训练集大小: 217561\n",
      "\n",
      "数据集的特征：\n",
      "{'dynasty': Value(dtype='string', id=None), 'author': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 指定数据集缓存目录\n",
    "cache_dir = \"/raid/gfc/llm/datasets/ChinesePoems\"\n",
    "\n",
    "# 加载数据集\n",
    "ds = load_dataset(\"larryvrh/Chinese-Poems\", cache_dir=cache_dir)\n",
    "\n",
    "# 查看数据集的基本信息\n",
    "print(\"数据集结构：\")\n",
    "print(ds)\n",
    "\n",
    "# 获取数据集的大小\n",
    "print(f\"\\n训练集大小: {len(ds['train'])}\")\n",
    "\n",
    "# 查看数据集的列名（特征）\n",
    "print(\"\\n数据集的特征：\")\n",
    "print(ds['train'].features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 如果数据集未处理，对数据集进行处理\n",
    "save_path = \"/raid/gfc/llm/datasets/ChinesePoems/poems.txt\"\n",
    "if not os.path.exists(save_path):\n",
    "    poems = []\n",
    "\n",
    "    for poem in ds['train']:\n",
    "        content = poem['content']\n",
    "        poems.append(content.replace('\\n', ''))\n",
    "\n",
    "\n",
    "    # 将诗句写入文件\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for poem in poems:\n",
    "            f.write(poem + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 制作 Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = f.readlines()\n",
    "        self.data = [line.strip() for line in self.data]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217561 青鞋踏尽剑鋩山，借枕僧房落照间。高屋凭虚听泉语，岭云应似我身闲。\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "dataset = MyDataset(save_path)\n",
    "\n",
    "print(len(dataset), dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# GPU选择函数\n",
    "def pick_free_gpu(start=7, end=0, memory_threshold=100):\n",
    "    \"\"\"\n",
    "    自动选择空闲的GPU\n",
    "    :param start: 起始GPU编号\n",
    "    :param end: 结束GPU编号\n",
    "    :param memory_threshold: 显存占用阈值（MB），低于此值认为GPU空闲\n",
    "    :return: torch.device对象\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 获取nvidia-smi输出，包含显存使用和GPU利用率\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used,utilization.gpu', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        # 解析输出\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            memory, util = map(int, line.split(', '))\n",
    "            gpu_info.append((memory, util))\n",
    "        \n",
    "        print(\"当前GPU状态：\")\n",
    "        for i, (memory, util) in enumerate(gpu_info):\n",
    "            print(f\"GPU {i}: 显存使用 {memory}MB, 利用率 {util}%\")\n",
    "        \n",
    "        # 从start到end检查GPU（包括end）\n",
    "        for i in range(start, end-1, -1):\n",
    "            if 0 <= i < len(gpu_info):  # 确保i在有效范围内\n",
    "                memory_used, gpu_util = gpu_info[i]\n",
    "                print(f\"检查GPU {i}: 显存使用 {memory_used}MB, 利用率 {gpu_util}%\")\n",
    "                # 判断条件：显存占用低于阈值且GPU利用率接近0\n",
    "                if memory_used < memory_threshold and gpu_util < 5:\n",
    "                    print(f\"选择空闲GPU: cuda:{i}\")\n",
    "                    print(f\"显存占用: {memory_used}MB, GPU利用率: {gpu_util}%\")\n",
    "                    return torch.device(f\"cuda:{i}\")\n",
    "        \n",
    "        print(\"没有检测到空闲GPU，使用CPU。\")\n",
    "        return torch.device(\"cpu\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"检测GPU时出错：{e}，使用CPU。\")\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前GPU状态：\n",
      "GPU 0: 显存使用 13421MB, 利用率 71%\n",
      "GPU 1: 显存使用 20777MB, 利用率 59%\n",
      "GPU 2: 显存使用 12861MB, 利用率 5%\n",
      "GPU 3: 显存使用 3MB, 利用率 0%\n",
      "GPU 4: 显存使用 20341MB, 利用率 0%\n",
      "GPU 5: 显存使用 3MB, 利用率 0%\n",
      "GPU 6: 显存使用 3MB, 利用率 0%\n",
      "GPU 7: 显存使用 3MB, 利用率 0%\n",
      "检查GPU 7: 显存使用 3MB, 利用率 0%\n",
      "选择空闲GPU: cuda:7\n",
      "显存占用: 3MB, GPU利用率: 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '这是很久之前的事情了 ， 但 是 从 去 年 开 始 它 就 慢 慢 失 去 了 兴 趣 ， 我 开 始 慢 慢 发 现 自 己 在 那 方 面 不 足 ， 那 个 时 候 看 起 来 ， 我 感 觉 不 是 在 那 个 地 方 做 了 什 么 努 力 ， 不 努 力 才 是 最 重 要 的 。 在 我 的 坚 持 下 ， 一 些 工 作 也 慢 慢 变 的 更 加 努 力 了'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "device = pick_free_gpu()\n",
    "\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\", cache_dir=cache_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\", cache_dir=cache_dir).to(device)\n",
    "text_generator = TextGenerationPipeline(model, tokenizer,device=device)   \n",
    "text_generator(\"这是很久之前的事情了\", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13597\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    data = tokenizer.batch_encode_plus(batch, \n",
    "                                       padding=True, \n",
    "                                       truncation=True, \n",
    "                                       max_length=512, \n",
    "                                       return_tensors=\"pt\")\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "    return data\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=16, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "Epochs = 30\n",
    "\n",
    "# 定义训练函数\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Epochs)\n",
    "    # 梯度裁剪\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # 记录最佳模型\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for i, batch in enumerate(loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "              \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    # 计算准确率\n",
    "                    model.eval()\n",
    "                    labels = batch['labels'][:, 1:].to(device)\n",
    "                    out = outputs.logits.argmax(dim=2)[:, :-1]\n",
    "\n",
    "                    select = labels != 0\n",
    "                    labels = labels[select]\n",
    "                    out = out[select]\n",
    "                    \n",
    "\n",
    "                    acc = (labels == out).sum().item() / labels.numel()\n",
    "                    lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch}, Step {i}, Lr {lr:.5e}, Loss {loss:.5f}, Acc {acc:.2%}\")\n",
    "\n",
    "                    del select\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # 每个epoch结束后的操作\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.5f}\")\n",
    "\n",
    "        # 调整学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        # 保存模型\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            model_save_path = \"/raid/gfc/llm/params/gpt_project\"\n",
    "            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, os.path.join(model_save_path, \"best_model.pt\"))         \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train() # 全量微调gpt2！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比原装pipeline 和 自定义pipeline的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 使用原始数据集进行训练的效果，发现数据集里面很多脏数据。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功从/raid/gfc/llm/params/gpt_project/best_model.pt加载参数\n",
      "========================================================================================\n",
      "模型结构如下\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(21128, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
      ")\n",
      "========================================================================================\n",
      "=== 原装pipeline微调后模型生成结果 ===\n",
      "\n",
      "输入: 天高\n",
      "输出: 天高 风 急 ， 白 云 卷 尽 不 成 云 。 野 烟 迷 野 树 ， 溪 流 乱 松 。 客 子 伤 行 役 ， 天 涯 悲 旅 魂 。 天 涯 何 处 是 ， 落 日 照 孤 村 。\n",
      "\n",
      "输入: 床前明月光\n",
      "输出: 床前明月光 ， 照 我 离 愁 千 万 斛 。 清 光 万 斛 ， 只 有 相 思 一 斛 。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "\n",
    "device = torch.device(\"cuda:4\")\n",
    "\n",
    "# 加载原装模型和tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\", cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\", cache_dir=cache_dir).to(device)\n",
    "\n",
    "finetuned_model_weights_path = \"/raid/gfc/llm/params/gpt_project/best_model.pt\"\n",
    "# 加载保存的模型参数\n",
    "checkpoint = torch.load(finetuned_model_weights_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"成功从{finetuned_model_weights_path}加载参数\")\n",
    "\n",
    "print(\"=\" * 88)\n",
    "print(\"模型结构如下\")\n",
    "print(model)\n",
    "print(\"=\" * 88)\n",
    "\n",
    "original_pipeline = TextGenerationPipeline(model, tokenizer, device=device)\n",
    "\n",
    "# 测试一些古诗文开头\n",
    "test_prompts = [\n",
    "    \"天高\",\n",
    "    \"床前明月光\"\n",
    "]\n",
    "\n",
    "# 原装pipeline\n",
    "print(\"=== 原装pipeline微调后模型生成结果 ===\")\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n输入: {prompt}\")\n",
    "    result = original_pipeline(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
    "    print(f\"输出: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS] 天 高 [SEP] [PAD] [PAD] ， 匆 吩植 mp4 枢 。 safari 池 kevin 蹣jing ， 猥俨亨縝 引 。\n",
      "1 [CLS] 床 前 明 月 光 ， 苦墉 オーフン5 粕橙 。什mark嘗 niusnews 混 ， sars遗チ才 谣 。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 自定义古诗词生成 Pipeline 生成五言绝句\n",
    "def PoetryGenerationPipeline(text, row, col):\n",
    "    # text提示词，row行数，col每一行字符数\n",
    "    # 定义一个递归函数，用于生成文本\n",
    "    def generate_text(data):\n",
    "        with torch.no_grad():\n",
    "            # print(data.input_ids.shape) # [batch_size, seq_len]\n",
    "            out = model(**data) # [batch_size, seq_len, hidden_size]\n",
    "            # for k in out.keys():\n",
    "            #     print(k) # loss logits past_key_values\n",
    "            # print(out.logits.shape) # torch.Size([2, 3, 21128])\n",
    "\n",
    "            # 得到最后一个字符的预测概率，因为第二维原本是3，我要预测下一个字符，所以只取最后一个\n",
    "            # out = out.logits\n",
    "            # out = out[:, -1] # torch.Size([2, 21128])\n",
    "            # 这两行代码和下面作用一样的，下面这个更清晰\n",
    "            last_token_prob = out.logits[:, -1, :] # torch.Size([2, 21128])\n",
    "            # print(out.shape)\n",
    "\n",
    "            top_k_values = torch.topk(last_token_prob, 50).values\n",
    "            # print(top_k_values.shape) # [batch_size, 50]\n",
    "            # print(top_k_values)\n",
    "            # 获取每个输出序列中前50个最大的logits（为保持维度不变，需要增加一个维度）\n",
    "            top_k_values = top_k_values[:,-1].unsqueeze(dim=1)\n",
    "            # print(top_k_values.shape) # [batch_size, 1]\n",
    "            # print(top_k_values)\n",
    "            \n",
    "            # 屏蔽低概率词\n",
    "            last_token_prob = last_token_prob.masked_fill(last_token_prob < top_k_values, -float('inf'))\n",
    "\n",
    "            # 屏蔽特殊符号\n",
    "            for sign in \"，。、；‘【】、《》？：“{}|,./;'[]?{}`~@\\\\#￥%……&*（）——+!@#$%^&*()_+\":\n",
    "                if sign in tokenizer.get_vocab():\n",
    "                    out.logits[:, :, tokenizer.get_vocab()[sign]] = -float('inf')\n",
    "            # 屏蔽字母\n",
    "            for letter in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "                if letter in tokenizer.get_vocab():\n",
    "                    out.logits[:, :, tokenizer.get_vocab()[letter]] = -float('inf')\n",
    "            # 屏蔽数字\n",
    "            for number in \"0123456789\":\n",
    "                if number in tokenizer.get_vocab():\n",
    "                    out.logits[:, :, tokenizer.get_vocab()[number]] = -float('inf')\n",
    "\n",
    "            # 采样，无放回，避免生成重复内容\n",
    "            last_token_prob = last_token_prob.softmax(dim=1) # torch.Size([2, 21128]) 得到归一化概率\n",
    "\n",
    "            last_token_prob = last_token_prob.softmax(dim=1)  # [2, 21128]\n",
    "            c = data['input_ids'].shape[1] / (1 + col)\n",
    "            if c % 1 == 0:\n",
    "                last_token_prob.fill_(0)  # 将所有概率清零\n",
    "                if c % 2 == 0:\n",
    "                    last_token_prob[:, tokenizer.get_vocab()[\"。\"]] = 1.0  # 强制生成 \"。\"\n",
    "                else:\n",
    "                    last_token_prob[:, tokenizer.get_vocab()[\"，\"]] = 1.0  # 强制生成 \"，\"\n",
    "\n",
    "            last_token_prob = last_token_prob.multinomial(num_samples=1) # torch.Size([2, 1]) 采样\n",
    "\n",
    "            # # 添加标点符号\n",
    "            # c = data['input_ids'].shape[1] / (1 + col)\n",
    "            # if c % 1 == 0:\n",
    "            #     if c % 2 == 0:\n",
    "            #         # 表示将第一个位置（索引0）的概率设为1，我们强制模型在这个位置生成对应的标点符号\n",
    "            #         last_token_prob[:,0] = tokenizer.get_vocab()[\"。\"]\n",
    "            #     else:\n",
    "            #         last_token_prob[:,0] = tokenizer.get_vocab()[\"，\"]\n",
    "\n",
    "            # 更新 input_ids，将新产生的词添加到输入序列中\n",
    "            data['input_ids'] = torch.cat([data['input_ids'], last_token_prob], dim=1)\n",
    "            data['attention_mask'] = torch.ones_like(data['input_ids'])\n",
    "            data['token_type_ids'] = torch.zeros_like(data['input_ids'])\n",
    "            data['labels'] = data['input_ids'].clone()\n",
    "            if data['input_ids'].shape[1] >= row * col + row + 1:\n",
    "                return data\n",
    "            return generate_text(data)\n",
    "        \n",
    "    # 测试\n",
    "    # 1. 对输入文本进行编码\n",
    "    # 得到[batch_size, seq_len]的一个矩阵，其中seq_len = [CLS] + token字符数\n",
    "    data = tokenizer.batch_encode_plus(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # 2. 移除编码后的序列中最后一个结束符号token\n",
    "    data['input_ids'] = data['input_ids'][:, :-1].to(device)\n",
    "    # 3. 创建与input_ids同形状的attention_mask\n",
    "    data['attention_mask'] = torch.ones_like(data['input_ids']).to(device)\n",
    "    # 4. 创建与input_ids同形状的token_type_ids\n",
    "    data['token_type_ids'] = torch.zeros_like(data['input_ids']).to(device)\n",
    "    # 5. 创建与input_ids同形状的labels\n",
    "    data['labels'] = data['input_ids'].clone().to(device)\n",
    "    data = generate_text(data)\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        print(i, tokenizer.decode(data[\"input_ids\"][i]))\n",
    "\n",
    "PoetryGenerationPipeline(test_prompts, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()[\"。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
