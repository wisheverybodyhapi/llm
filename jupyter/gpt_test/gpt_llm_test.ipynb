{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试调用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 测试文本生成大模型（中文小型语料库，包含新闻、评论）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/gfc/anaconda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '你好 ！ 小 王 说 ： 我 们 要 把 你 的 孩 子 打 下 来 ， 把 孩 子 的 情 况 告 诉 我 们 。 我 们 要 把 孩 子 打 下 来 ， 把 孩 子 的 情 况 告 诉 我 们 ， 让 他 们 知 道 ， 如 果 他 们 的 情 况 是 这 样 的 ， 他 们 就 会 很 快 的 好 起 来 。 孩 子 会 很 快 乐 的 成 长 。 我 们 要 做'}]\n"
     ]
    }
   ],
   "source": [
    "# 1. 导包\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# 2. 加载模型和分词器\n",
    "# 设置设备为 cuda:7\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_dir = \"/raid/gfc/llm/models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# 3. 使用加载的模型和分词器创建生成文本的pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=7)\n",
    "\n",
    "# 4. 生成文本\n",
    "output = generator(\n",
    "    \"你好\",\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    truncation=True,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 歌词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '最美的不是下雨天，是曾与你躲过雨的屋檐 ， 我 们 的 爱 情 ， 就 像 那 ， 一 张 张 泛 黄 的 照 片 ， 我 们 的 爱 情 ， 就 像 那 ， 一 本 本 泛 黄 的 书 ， 我 们 的 爱 情 ， 就 像 那 ， 一 本 本 泛 黄 的 书 ， 我 们 的 爱 情 ， 就 像 那 ， 一 本 本 泛 黄 的 书 ， 我 们 的 爱 情 ， 就'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 指定模型下载目录\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\", cache_dir=cache_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-lyric\", cache_dir=cache_dir).to(device)\n",
    "# 在创建pipeline时直接指定device参数\n",
    "text_generator = TextGenerationPipeline(model, tokenizer, device=7)\n",
    "# do_sample=True 每次都不一样 do_sample=False 每次都一样\n",
    "text_generator(\"最美的不是下雨天，是曾与你躲过雨的屋檐\", max_length=100, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 古诗文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '当是时 ， 吾 不 复 食 卿 肉 。 ” 时 上 遂 即 以 是 年 八 十 三 而 卒 ， 时 年 八 十 一 。 元 寿 元 年 也 。 张 安 世 在 位 后 五 十 年 而 魏 将 黄 门 郎 史 高 为 卫 司 马 军 长 史 ， 与 司 马 懿 俱 入 阁 ， 共 秉 朝 政 ， 时 号 三 公 。 司 马 氏 之 女 名 安 ， 与 安 相 继 为 上 官 氏 妻'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 指定模型下载目录\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-ancient\", cache_dir=cache_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-ancient\", cache_dir=cache_dir).to(device)\n",
    "# 在创建pipeline时直接指定device参数\n",
    "text_generator = TextGenerationPipeline(model, tokenizer, device=7)\n",
    "text_generator(\"当是时\", max_length=100, do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 对联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '[CLS]天气晚来秋 ， 红 枫 怒 放 一 山 火 - 霜 威 先 闪 电 ， 黄 叶 轻 扬'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 指定模型下载目录\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-couplet\", cache_dir=cache_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-couplet\", cache_dir=cache_dir).to(device)\n",
    "# 在创建pipeline时直接指定device参数\n",
    "text_generator = TextGenerationPipeline(model, tokenizer, device=7)\n",
    "text_generator(\"[CLS]天气晚来秋\", max_length=25, do_sample=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
