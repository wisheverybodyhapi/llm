{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载qwen模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型加载完成\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from modelscope import snapshot_download\n",
    "from time import sleep\n",
    "\n",
    "# 设置下载缓存目录\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "\n",
    "# 下载模型并返回本地路径\n",
    "cache_dir = snapshot_download('Qwen/Qwen2.5-1.5B-Instruct', cache_dir=cache_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 使用从 ModelScope 下载的模型和 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(cache_dir)\n",
    "model.to(device)\n",
    "print(\"模型加载完成\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Assume parameters are in float16 (2 bytes each) or float32 (4 bytes each)\n",
    "# Qwen2.5-1.5B-Instruct typically uses float16 by default\n",
    "bytes_per_param = 2  # float16\n",
    "total_bytes = total_params * bytes_per_param\n",
    "print(f\"Total bytes: {total_bytes:,}\")\n",
    "\n",
    "# Calculate GPU memory in GB\n",
    "total_memory_gb = total_bytes / (1024**3)\n",
    "print(f\"GPU memory required: {total_memory_gb:.2f} GB\")\n",
    "\n",
    "sleep(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 660/660 [00:00<00:00, 904B/s]\n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 5.97B/s]\n",
      "Downloading: 100%|██████████| 242/242 [00:00<00:00, 606B/s]\n",
      "Downloading: 100%|██████████| 11.1k/11.1k [00:00<00:00, 28.9kB/s]\n",
      "Downloading: 100%|██████████| 1.59M/1.59M [00:00<00:00, 2.93MB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "snapshot_download('Qwen/Qwen2.5-1.5B-Instruct', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
