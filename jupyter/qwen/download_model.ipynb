{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载qwen模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM, snapshot_download\n",
    "import torch\n",
    "\n",
    "cache_dir = \"/raid/gfc/llm/models\"\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "model_dir = snapshot_download(cache_dir=cache_dir, model_id=model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>\n",
      "['ä½łå¥½', 'åĲĹ', 'ï¼Ł', 'å®Ŀè´Ŀ']\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))\n",
    "print(tokenizer.tokenize(\"你好吗？宝贝\")) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [108386, 101037, 11319, 105882]\n",
      "decoded: 你好吗？宝贝\n"
     ]
    }
   ],
   "source": [
    "text = \"你好吗？宝贝\"\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(\"input_ids:\", input_ids) # input_ids: [108386, 101037, 11319, 105882]\n",
    "\n",
    "# 解码回来看看是否一致\n",
    "decoded = tokenizer.decode(input_ids)\n",
    "print(\"decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [108386, 101037, 11319, 105882]\n"
     ]
    }
   ],
   "source": [
    "text = \"你好吗？宝贝\"\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True) # input_ids: [108386, 101037, 11319, 105882]\n",
    "print(\"input_ids:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token, tokenizer.eos_token)  # None None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"<|im_start|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "(torch.tensor([1, 2, 3]) + torch.tensor([4, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  35127,    752,    264,\n",
      "           2805,  16800,    311,   3460,   4128,   1614,     13, 151645,    198,\n",
      "         151644,  77091,    198]], device='cuda:7')\n",
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  35127,    752,    264,\n",
      "           2805,  16800,    311,   3460,   4128,   1614,     13, 151645,    198,\n",
      "         151644,  77091,    198,   2121,    458,  15235,   7881,    553,  54364,\n",
      "          14817,     11,    358,   1079,   2598,   1207,  16948,     13,   3017,\n",
      "           7428,    374,    311,   7789,   3847,    304,  23163,   1467,   3118,\n",
      "            389,  50932,   3897,    311,    752,     13,    358,    646,   1492,\n",
      "            448,   4378,  22844,     11,  75878,  44219,     11,   6825,   7343,\n",
      "             11,    323,   1496,  35764,   4755,    911,   5257,  13347,     13,\n",
      "            358,  36006,    311,   3410,  13382,    323,   9760,   1995,    311,\n",
      "            847,   3847,   1393,  20337,    279,   8426,  10659,    315,  30208,\n",
      "           6786,     13, 151645]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs.input_ids)\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091, 198]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_start|>assistant\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  2121,    458,  15235,   7881,    553,  54364,  14817,     11,    358,\n",
       "           1079,   2598,   1207,  16948,     13,   3017,   7428,    374,    311,\n",
       "           7789,   3847,    304,  23163,   1467,   3118,    389,  50932,   3897,\n",
       "            311,    752,     13,    358,    646,   1492,    448,   4378,  22844,\n",
       "             11,  75878,  44219,     11,   6825,   7343,     11,    323,   1496,\n",
       "          35764,   4755,    911,   5257,  13347,     13,    358,  36006,    311,\n",
       "           3410,  13382,    323,   9760,   1995,    311,    847,   3847,   1393,\n",
       "          20337,    279,   8426,  10659,    315,  30208,   6786,     13, 151645],\n",
       "        device='cuda:7')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_prompt = tokenizer.decode(model_inputs.input_ids[0])\n",
    "print(decoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI developed by Alibaba Cloud, I am called Qwen. My purpose is to assist users in generating text based on prompts provided to me. I can help with writing essays, composing poems, creating stories, and even answering questions about various topics. I strive to provide accurate and relevant information to my users while maintaining the highest standards of ethical conduct.<|im_end|>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI developed by Alibaba Cloud, I am called Qwen. My purpose is to assist users in generating text based on prompts provided to me. I can help with writing essays, composing poems, creating stories, and even answering questions about various topics. I strive to provide accurate and relevant information to my users while maintaining the highest standards of ethical conduct.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "instruction = tokenizer(\n",
    "    f\"<|im_start|>system\\n你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型<|im_end|>\\n<|im_start|>user\\n我感冒了怎么办？<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "response = tokenizer(f\"感冒了，睡觉就好了！\", add_special_tokens=False)\n",
    "input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "attention_mask = (instruction[\"attention_mask\"] + response[\"attention_mask\"])\n",
    "print(attention_mask == instruction[\"attention_mask\"] + response[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
