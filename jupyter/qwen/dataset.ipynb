{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e7b51f",
   "metadata": {},
   "source": [
    "# 下载数据集，并对数据集进行处理！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d8b82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f7952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/gfc/anaconda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集结构：\n",
      "train: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'history'],\n",
      "    num_rows: 799743\n",
      "})\n",
      "\n",
      "训练集大小: 799743\n",
      "\n",
      "数据集的特征：\n",
      "{'instruction': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None), 'history': Value(dtype='null', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"/raid/gfc/llm/datasets/Chinese-medical-dialogue\"\n",
    "ds = load_dataset(\"ticoAg/Chinese-medical-dialogue\", cache_dir=dataset_path)\n",
    "\n",
    "# 查看数据集的基本信息\n",
    "print(\"数据集结构：\")\n",
    "for k, v in ds.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# 获取数据集的大小\n",
    "print(f\"\\n训练集大小: {len(ds['train'])}\")\n",
    "\n",
    "# 查看数据集的列名（特征）\n",
    "print(\"\\n数据集的特征：\")\n",
    "print(ds['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8f3fc",
   "metadata": {},
   "source": [
    "### 2. 清洗数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37186d15",
   "metadata": {},
   "source": [
    "##### 2.1 缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32fe0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始样本数: 799743\n",
      "清洗后样本数: 799736\n",
      "{'instruction': '小儿肥胖超重该如何治疗', 'input': '女宝宝，刚7岁，这一年，察觉到，我家孩子身上肉很多，而且，食量非常的大，平时都不喜欢吃去玩，请问：小儿肥胖超重该如何治疗。', 'output': '孩子出现肥胖症的情况。家长要通过孩子运功和健康的饮食来缓解他的症状，可以先让他做一些有氧运动，比如慢跑，爬坡，游泳等，并且饮食上孩子多吃黄瓜，胡萝卜，菠菜等，禁止孩子吃一些油炸食品和干果类食物，这些都是干热量高脂肪的食物，而且不要让孩子总是吃完就躺在床上不动，家长在治疗小儿肥胖期间如果孩子情况严重就要及时去医院在医生的指导下给孩子治疗。', 'history': None}\n",
      "小儿肥胖超重该如何治疗\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 数据清洗\n",
    "def is_valid(example):\n",
    "    # 三项都不能为空且不是纯空格\n",
    "    return all([\n",
    "        example['instruction'] and example['instruction'].strip(),\n",
    "        example['input'] and example['input'].strip(),\n",
    "        example['output'] and example['output'].strip()\n",
    "    ])\n",
    "\n",
    "dataset = ds['train']\n",
    "print(f\"原始样本数: {len(dataset)}\")\n",
    "\n",
    "# 过滤空值\n",
    "ds_clean = dataset.filter(is_valid)\n",
    "print(f\"清洗后样本数: {len(ds_clean)}\")\n",
    "\n",
    "print(ds_clean[0])\n",
    "print(ds_clean[0]['instruction'])\n",
    "print(type(ds_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530ba55",
   "metadata": {},
   "source": [
    "##### 2.2 格式化规范\n",
    "去除多余空格、特殊符号。\n",
    "统一全角/半角、简繁体（如有需要）。\n",
    "统一标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f036a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据格式化完成！\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jaconv\n",
    "from zhconv import convert\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9，。！？、；：“”‘’（）《》【】]', '', text)\n",
    "    text = jaconv.z2h(text, kana=False, ascii=True, digit=True)\n",
    "    text = convert(text, 'zh-cn')\n",
    "    return text\n",
    "\n",
    "def normalize_example(example):\n",
    "    for col in ['instruction', 'input', 'output']:\n",
    "        example[col] = normalize_text(str(example[col]))\n",
    "    return example\n",
    "\n",
    "# 推荐：直接在 HuggingFace Dataset 上并行处理\n",
    "ds_clean = ds_clean.map(normalize_example, num_proc=4)  # 可根据CPU核数调整num_proc\n",
    "\n",
    "print(\"数据格式化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55df9c5",
   "metadata": {},
   "source": [
    "##### 2.3 去除冗余数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76037d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重后样本数: 752436\n"
     ]
    }
   ],
   "source": [
    "# 去重（以instruction+input+output为唯一标识）\n",
    "import pandas as pd\n",
    "from simhash import Simhash\n",
    "\n",
    "ds_clean = ds_clean.to_pandas()\n",
    "ds_clean = ds_clean.drop_duplicates(subset=['instruction', 'input', 'output'])\n",
    "print(f\"去重后样本数: {len(ds_clean)}\")\n",
    "\n",
    "# # 近似去重示例（SimHash）\n",
    "# def get_simhash(text):\n",
    "#     return Simhash(text).value\n",
    "\n",
    "# ds_clean['simhash'] = ds_clean.apply(lambda row: get_simhash(row['instruction'] + row['input'] + row['output']), axis=1)\n",
    "# ds_clean = ds_clean.drop_duplicates(subset=['simhash'])\n",
    "# print(f\"近似去重后样本数: {len(ds_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6110b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "格式化后的数据集已保存到: /raid/gfc/llm/datasets/Chinese-medical-dialogue/formatted_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 存储格式化以后的数据集\n",
    "formated_dataset_path = os.path.join(dataset_path, \"formatted_dataset.csv\")\n",
    "\n",
    "ds_clean.to_csv(formated_dataset_path, index=False, encoding='utf-8')\n",
    "print(f\"格式化后的数据集已保存到: {formated_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9786a5",
   "metadata": {},
   "source": [
    "##### 2.4 异常样本过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc38ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 可选：过滤过短/过长\n",
    "# ds_clean = pd.read_csv(formated_dataset_path, encoding='utf-8')\n",
    "\n",
    "# # 拼接instruction和input\n",
    "# ds_clean['prompt'] = ds_clean['instruction'].astype(str) + ds_clean['input'].astype(str)\n",
    "\n",
    "# # exceptions_idx = ds_clean[ds_clean['prompt'].str.len() < 5].index\n",
    "# # print(ds_clean.iloc[exceptions_idx[0]])\n",
    "# # 过滤拼接后长度过短的样本\n",
    "# ds_clean = ds_clean[ds_clean['prompt'].str.len() >= 5]\n",
    "\n",
    "# print(f\"过滤过短/过长后的样本数: {len(ds_clean)}\")\n",
    "\n",
    "# # 保存清洗后的数据\n",
    "# cleaned_dataset_path = os.path.join(dataset_path, \"cleaned_dataset.csv\")\n",
    "# ds_clean.to_csv(cleaned_dataset_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05369c",
   "metadata": {},
   "source": [
    "##### 2.4 语义级别去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae44a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert模型加载完成！\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_cache_dir = \"/raid/gfc/llm/models\"\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-chinese\", cache_dir=bert_cache_dir).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\", cache_dir=bert_cache_dir)\n",
    "\n",
    "print(\"bert模型加载完成！\")\n",
    "# print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9364f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语义级别去重\n",
    "from torch.utils.data import DataLoader\n",
    "import faiss\n",
    "\n",
    "# 1. 读取数据\n",
    "df = pd.read_csv(formated_dataset_path, encoding='utf-8')\n",
    "\n",
    "# 2. 拼接文本\n",
    "texts = (df['instruction'].astype(str) + '[SEP]' + df['input'].astype(str) + '[SEP]' + df['output'].astype(str)).tolist()\n",
    "# texts = texts[:100]  # 测试，仅处理前100条数据，实际使用时可去掉此行\n",
    "# for text in texts:\n",
    "#     print(text)\n",
    "\n",
    "def batch_get_bert_embedding(texts, tokenizer, model, device, batch_size=64):\n",
    "    all_embeds = []\n",
    "    loader = DataLoader(texts, batch_size=batch_size, shuffle=False)\n",
    "    for batch in tqdm(loader, desc=\"BERT批量提取嵌入\"):\n",
    "        inputs = tokenizer(\n",
    "            list(batch), \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=True\n",
    "        )\n",
    "        # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        # print(inputs['input_ids'].shape)  # [batch_size, seq_len]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # print(outputs.keys()) # dict_keys(['last_hidden_state', 'pooler_output'])\n",
    "            # print(outputs.last_hidden_state.shape)  # [batch_size, seq_len, hidden_size]\n",
    "            # print(outputs.pooler_output.shape)  # [batch_size, hidden_size]\n",
    "            # print(outputs.pooler_output.shape == outputs.last_hidden_state[:, 0, :].shape) # True\n",
    "            # print(outputs.pooler_output == outputs.last_hidden_state[:, 0, :]) # False\n",
    "            # 取[CLS]的输出作为句子嵌入\n",
    "            embeds = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeds.append(embeds)\n",
    "        \n",
    "    return np.vstack(all_embeds)\n",
    "\n",
    "# 使用方法\n",
    "embeddings = batch_get_bert_embedding(texts, tokenizer, bert_model, device, batch_size=256)\n",
    "\n",
    "# 存储embddings\n",
    "embeddings_path = os.path.join(dataset_path, \"embeddings.npy\")\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"嵌入已保存到: {embeddings_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43aeabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  instruction  \\\n",
      "70000       请问孩子出生2个多月稀大便怎么办?   \n",
      "70001  拉肚子瞧了好几回都是好两天还拉好两天怎么办?   \n",
      "70002         宝宝七个月一直拉肚子怎么治疗?   \n",
      "70003      我宝宝两周岁了腿还软不能走路怎么办?   \n",
      "70004          孩子老清嗓子还痰多怎么回事?   \n",
      "70005            入睡后汗特别多怎么办呢?   \n",
      "70006        五岁的宝宝鼻子总爱出血怎么办呢?   \n",
      "70007           7岁的男孩会有可能手淫吗?   \n",
      "70008            我儿子是否得了病毒感染?   \n",
      "70009           小儿癫痫要做哪些仔细检查?   \n",
      "\n",
      "                                                   input  \\\n",
      "70000  您好孩子降生一个多月后到现在2个多月一直是35解一次稀大便,放屁多又臭吃得少,昼夜一惊一诈怎么办?   \n",
      "70001  宝宝这几天一直拉肚子瞧了好几回都是好两天还拉好两天还拉,吃晚饭挺正常的也玩就是拉肚子,一天两...   \n",
      "70002  宝宝七个月!一直拉肚子后就换了乳酸菌奶粉?吃后不拉拉!不久后又拉白色粑粑和绿色的!有时还吃啥...   \n",
      "70003  两周岁了腿软不能够走路!下地学走路一直是点起脚尖走!曾经的治疗情况和效果:无在乎怎样的帮助:...   \n",
      "70004  孩子老是清嗓子,不是发烧,是不是抽动症啊。河南郑州哪里可以治疗吗?郑州建设东路24那个医院怎...   \n",
      "70005  宝宝入眠治好汗特别多,每次都是,现在天气不是很热,宝宝穿的衣服也并不多,怎么会出这么多汗呢,...   \n",
      "70006  我的孩子五岁了,经常爱流鼻血,流鼻血成了我孩子的家常便饭,他爱用手去抠鼻子,也不知晓他的鼻子...   \n",
      "70007  近来发觉孩子一直往卫生间跑,在里面一呆就是好长时间。一次偷偷看一看他在干什么。发觉他正用手在...   \n",
      "70008  孩子是从前几天已经开始不舒服的,从幼儿园回去以后,就有点不舒服,晚上已经开始发高烧,吃了退烧...   \n",
      "70009  朋友亲戚的孩子,出现过几次癫痫症状,有时活动正在进行时,她突然话语中断,双眼茫然的不知道在看...   \n",
      "\n",
      "                                                  output  \\\n",
      "70000  如果宝宝是母乳喂养的一天三到五次大便也进正常的,如果是混和喂养的宝宝再次出现上述的情况考虑是...   \n",
      "70001  婴儿消化系统生长发育不成熟,胃酸和消化酶的排泄少,消化酶的活性低,忍受没法饮食物质和量的较大...   \n",
      "70002  很高兴为你解惑,根据你购买的情况,考虑为肠胃功能发育不良,致使消化不良。不必要紧张,慢慢调养...   \n",
      "70003  二岁的孩子腿软还不能够走路的现象,要仔细检查微量元素,看如何有缺钙,要给孩子消化治疗的。指导...   \n",
      "70004  孩子老是有清嗓子的现象,如果没发烧,要仔细检查看如何有咽炎的可能会,要治疗的。指导意见:再个...   \n",
      "70005  小儿多汗可以分成生理性多汗与病理性多汗两大类。生理性多汗常常可以寻到明显的原因,如衣服穿得过...   \n",
      "70006  不要让孩子用手去弄鼻子孩子流鼻血时,家长多让孩子仰首或用冰块冰敷孩子的额头来敷药,这样的行为...   \n",
      "70007  2岁的孩子应当还是好奇的多,性器官还没完全生长发育完整。家长应当正确引导。频繁性交可引发泌尿...   \n",
      "70008  反反复复发高烧,应该是身体有炎症,最好是做几下血常规仔细检查吧,如果是病毒感染,需用抗病毒口...   \n",
      "70009  小儿再次出现癫痫症状,脑电图是癫痫确诊的最基本、最重要的的工具,应该说每一个癫痫的确诊必须要...   \n",
      "\n",
      "                       keywords  \n",
      "70000  [胃肠功能, 考虑, 再次出现, 增强, 防寒]  \n",
      "70001   [经查, 饮食, 仔细检查, 药物, 助消化]  \n",
      "70002     [奶粉, 乳酸菌, 建议, 考虑, 肠胃]  \n",
      "70003      [走路, 孩子, 治疗, 现象, 能够]  \n",
      "70004     [治疗, 孩子, 河南, 抽动症, 期望]  \n",
      "70005  [多汗, 生理性, 看一看, 在乎, 检查一下]  \n",
      "70006     [孩子, 敷药, 流鼻血, 血液, 鼻子]  \n",
      "70007      [可能, 减退, 性交, 早泄, 期望]  \n",
      "70008    [病毒感染, 儿子, 体温, 消化, 降温]  \n",
      "70009    [癫痫, 仔细检查, 症状, 小儿, 出现]  \n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "def keybert_keywords(row, topk=5):\n",
    "    text = row['instruction'] + row['input'] + row['output']\n",
    "    keywords = kw_model.extract_keywords(text, top_n=topk)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "df['keywords'] = df.apply(keybert_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "embeddings从/raid/gfc/llm/datasets/Chinese-medical-dialogue/embeddings.npy加载完成！\n"
     ]
    }
   ],
   "source": [
    "# 5. 语义去重（两两余弦相似度，阈值可调，建议0.95~0.98）\n",
    "import faiss\n",
    "\n",
    "embeddings_path = os.path.join(dataset_path, \"embeddings.npy\")\n",
    "embeddings = np.load(embeddings_path)\n",
    "print(type(embeddings))  # <class 'numpy.ndarray'>\n",
    "print(f\"embeddings从{embeddings_path}加载完成！\")\n",
    "\n",
    "# 假设embeddings为np.ndarray, shape=(N, D)\n",
    "faiss.normalize_L2(embeddings)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "D, I = index.search(embeddings, k=10)  # k为每个向量查找的近邻数\n",
    "\n",
    "# 根据相似度阈值筛选重复样本\n",
    "threshold = 0.97\n",
    "visited = set()\n",
    "keep_idx = []\n",
    "for i in tqdm(len(embeddings)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    keep_idx.append(i)\n",
    "    for j, sim in zip(I[i], D[i]):\n",
    "        if j != i and sim > threshold:\n",
    "            visited.add(j)\n",
    "\n",
    "df_semantic_dedup = df.iloc[keep_idx].reset_index(drop=True)\n",
    "print(f\"语义去重后样本数: {len(df_semantic_dedup)}\")\n",
    "\n",
    "# 6. 保存\n",
    "df_semantic_dedup.to_csv(dataset_path + \"/semantic_dedup.csv\", index=False, encoding='utf-8')\n",
    "print(\"语义去重后的数据已保存。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13949b25",
   "metadata": {},
   "source": [
    "### 3. 处理数据集 转换成qwen指令微调的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53eb8607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共593437条数据\n"
     ]
    }
   ],
   "source": [
    "# 处理数据集成qwen微调所需的格式\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/raid/gfc/llm/datasets/Chinese-medical-dialogue\"\n",
    "\n",
    "formatted_dataset_path = os.path.join(dataset_path, \"formatted_dataset.csv\")\n",
    "dedup_idx_path = os.path.join(dataset_path, \"dedup_idx.npy\")\n",
    "formatted_dataset = pd.read_csv(formatted_dataset_path, encoding='utf-8')\n",
    "dataset_idx = np.load(dedup_idx_path)\n",
    "dataset = formatted_dataset.iloc[dataset_idx].reset_index(drop=True)\n",
    "print(f\"一共{len(dataset)}条数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba490c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 474749, 验证集: 59344, 测试集: 59344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = dataset\n",
    "# 打乱数据\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 先划分训练集和临时集\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# 再将临时集一分为二，得验证集和测试集\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"训练集: {len(train_df)}, 验证集: {len(val_df)}, 测试集: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e722c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集已保存为jsonl格式。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def to_prompt_response(row):\n",
    "    prompt = str(row[\"instruction\"]) + \"\\n\" + str(row[\"input\"])\n",
    "    response = str(row[\"output\"])\n",
    "    return {\"prompt\": prompt, \"response\": response}\n",
    "\n",
    "def save_jsonl(df, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            item = to_prompt_response(row)\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(train_df, f\"{dataset_path}/train.jsonl\")\n",
    "save_jsonl(val_df, f\"{dataset_path}/val.jsonl\")\n",
    "save_jsonl(test_df, f\"{dataset_path}/test.jsonl\")\n",
    "print(\"数据集已保存为jsonl格式。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18344e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
