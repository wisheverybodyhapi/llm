{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21af3869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 4])\n",
      "MultiHeadAttention attn_weights tensor([[[[0.3089, 0.3215, 0.1612, 0.2083],\n",
      "          [0.1645, 0.1572, 0.4000, 0.2783],\n",
      "          [0.3302, 0.3489, 0.1314, 0.1895],\n",
      "          [0.3323, 0.3729, 0.1244, 0.1704]],\n",
      "\n",
      "         [[0.2310, 0.2231, 0.2686, 0.2772],\n",
      "          [0.2469, 0.2644, 0.2496, 0.2391],\n",
      "          [0.3336, 0.1292, 0.1934, 0.3438],\n",
      "          [0.2311, 0.1544, 0.2663, 0.3482]]],\n",
      "\n",
      "\n",
      "        [[[0.2404, 0.3100, 0.2313, 0.2182],\n",
      "          [0.2458, 0.2169, 0.2768, 0.2605],\n",
      "          [0.2514, 0.2822, 0.2279, 0.2385],\n",
      "          [0.2003, 0.0730, 0.4141, 0.3125]],\n",
      "\n",
      "         [[0.2751, 0.2653, 0.2068, 0.2528],\n",
      "          [0.1778, 0.1424, 0.1709, 0.5089],\n",
      "          [0.2463, 0.2422, 0.2453, 0.2663],\n",
      "          [0.2540, 0.2341, 0.2031, 0.3087]]]], grad_fn=<SoftmaxBackward0>)\n",
      "MultiHeadAttention attn_output shape torch.Size([2, 2, 4, 2])\n",
      "MultiHeadAttention attn_output before concat tensor([[[[-0.0384,  0.9479],\n",
      "          [-0.0138,  1.4531],\n",
      "          [-0.0374,  0.8628],\n",
      "          [-0.0230,  0.8056]],\n",
      "\n",
      "         [[ 0.1091, -0.5924],\n",
      "          [ 0.0858, -0.5647],\n",
      "          [ 0.2909, -0.6818],\n",
      "          [ 0.1821, -0.6511]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0310,  1.2020],\n",
      "          [-0.0654,  1.1302],\n",
      "          [-0.0056,  1.2027],\n",
      "          [-0.2143,  0.9209]],\n",
      "\n",
      "         [[ 0.0922, -0.5875],\n",
      "          [ 0.6082, -0.6994],\n",
      "          [ 0.1333, -0.5835],\n",
      "          [ 0.2075, -0.6123]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "MultiHeadAttention attn_output after concat tensor([[[-0.0384,  0.9479,  0.1091, -0.5924],\n",
      "         [-0.0138,  1.4531,  0.0858, -0.5647],\n",
      "         [-0.0374,  0.8628,  0.2909, -0.6818],\n",
      "         [-0.0230,  0.8056,  0.1821, -0.6511]],\n",
      "\n",
      "        [[ 0.0310,  1.2020,  0.0922, -0.5875],\n",
      "         [-0.0654,  1.1302,  0.6082, -0.6994],\n",
      "         [-0.0056,  1.2027,  0.1333, -0.5835],\n",
      "         [-0.2143,  0.9209,  0.2075, -0.6123]]], grad_fn=<ViewBackward0>)\n",
      "多头注意力输出形状: torch.Size([2, 4, 4])\n",
      "多头注意力输出: [[[ 1.3480803  -0.9324167   0.5753625  -0.9910262 ]\n",
      "  [-0.3949206   1.7117761  -0.50549346 -0.81136197]\n",
      "  [ 0.33816335 -1.064641   -0.75216913  1.4786466 ]\n",
      "  [ 0.85967916 -1.2738004  -0.6740183   1.0881397 ]]\n",
      "\n",
      " [[ 0.9947721  -0.47707826 -1.4096704   0.89197654]\n",
      "  [ 0.3153217  -0.88179773 -0.93383384  1.5003098 ]\n",
      "  [-0.00695081  1.6031486  -0.51889735 -1.0773003 ]\n",
      "  [ 0.8551513  -1.6967652   0.5540064   0.2876075 ]]]\n",
      "SingleHeadAttention attn_weights tensor([[[0.2803, 0.2814, 0.1969, 0.2414],\n",
      "         [0.1879, 0.1909, 0.3549, 0.2663],\n",
      "         [0.3882, 0.2064, 0.1376, 0.2678],\n",
      "         [0.3125, 0.2548, 0.1724, 0.2603]],\n",
      "\n",
      "        [[0.2604, 0.3037, 0.2070, 0.2290],\n",
      "         [0.1987, 0.1555, 0.2102, 0.4356],\n",
      "         [0.2487, 0.2667, 0.2314, 0.2532],\n",
      "         [0.2242, 0.1037, 0.3198, 0.3524]]], grad_fn=<SoftmaxBackward0>)\n",
      "SingleHeadAttention attn_output shape torch.Size([2, 4, 4])\n",
      "SingleHeadAttention attn_output tensor([[[-0.0443,  1.0729,  0.1196, -0.5731],\n",
      "         [-0.0153,  1.3580,  0.0607, -0.5779],\n",
      "         [-0.1590,  1.1005,  0.2696, -0.6342],\n",
      "         [-0.0846,  1.0952,  0.1744, -0.5994]],\n",
      "\n",
      "        [[ 0.0201,  1.2358,  0.0323, -0.5639],\n",
      "         [-0.3589,  1.1418,  0.4747, -0.6658],\n",
      "         [-0.0339,  1.1923,  0.0967, -0.5753],\n",
      "         [-0.2509,  1.0364,  0.3626, -0.6334]]], grad_fn=<UnsafeViewBackward0>)\n",
      "单头注意力输出形状: torch.Size([2, 4, 4])\n",
      "单头注意力输出: [[[ 1.2757117  -0.9892332   0.6792091  -0.9656877 ]\n",
      "  [-0.38782674  1.7157257  -0.5539749  -0.7739241 ]\n",
      "  [ 0.33705616 -1.116683   -0.69080466  1.4704317 ]\n",
      "  [ 0.84379125 -1.333472   -0.58880794  1.0784886 ]]\n",
      "\n",
      " [[ 0.98421323 -0.4645673  -1.4175047   0.89785874]\n",
      "  [ 0.38189405 -0.8013871  -1.0400374   1.4595305 ]\n",
      "  [-0.01358037  1.6139667  -0.5611604  -1.0392259 ]\n",
      "  [ 0.8355167  -1.6906481   0.6249635   0.23016794]]]\n",
      "多头与单头输出差异（绝对值）: [[[0.07236862 0.05681652 0.10384661 0.02533853]\n",
      "  [0.00709385 0.00394952 0.04848146 0.03743786]\n",
      "  [0.00110719 0.05204201 0.06136447 0.00821495]\n",
      "  [0.01588792 0.05967164 0.08521038 0.00965106]]\n",
      "\n",
      " [[0.01055884 0.01251096 0.00783432 0.0058822 ]\n",
      "  [0.06657234 0.08041066 0.10620356 0.04077935]\n",
      "  [0.00662955 0.01081812 0.04226303 0.03807437]\n",
      "  [0.0196346  0.00611711 0.07095712 0.05743955]]]\n",
      "W_q 权重是否相同: True\n",
      "W_k 权重是否相同: True\n",
      "W_v 权重是否相同: True\n",
      "W_o 权重是否相同: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机数种子\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 位置编码模块\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 多头自注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        print(\"MultiHeadAttention attn_weights\",attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        print(\"MultiHeadAttention attn_output shape\",attn_output.shape)\n",
    "        print(\"MultiHeadAttention attn_output before concat\",attn_output)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        print(\"MultiHeadAttention attn_output after concat\",attn_output)\n",
    "        output = self.W_o(attn_output)\n",
    "        return output\n",
    "\n",
    "# 单头注意力模块\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        print(\"SingleHeadAttention attn_weights\",attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        print(\"SingleHeadAttention attn_output shape\",attn_output.shape)\n",
    "        print(\"SingleHeadAttention attn_output\",attn_output)\n",
    "        output = self.W_o(attn_output)\n",
    "        return output\n",
    "\n",
    "# 前馈网络\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Transformer层（多头）\n",
    "class MultiHeadTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(MultiHeadTransformerLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "# Transformer层（单头）\n",
    "class SingleHeadTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(SingleHeadTransformerLayer, self).__init__()\n",
    "        self.attention = SingleHeadAttention(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "# 简单Transformer模型\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, max_len=5000, use_multi_head=True):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.use_multi_head = use_multi_head\n",
    "        if use_multi_head:\n",
    "            self.transformer_layer = MultiHeadTransformerLayer(d_model, num_heads, d_ff)\n",
    "        else:\n",
    "            self.transformer_layer = SingleHeadTransformerLayer(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.pos_encoding(embedded)\n",
    "        output = self.transformer_layer(embedded)\n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 固定随机数种子\n",
    "    set_seed(42)\n",
    "\n",
    "    vocab_size = 100\n",
    "    d_model = 4\n",
    "    num_heads = 2\n",
    "    d_ff = 16\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "\n",
    "    # 输入数据\n",
    "    x = torch.tensor([[1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "    print(\"输入形状:\", x.shape)\n",
    "\n",
    "    # 多头注意力模型\n",
    "    set_seed(42)  # 确保每次初始化前种子一致\n",
    "    multi_head_model = SimpleTransformer(vocab_size, d_model, num_heads, d_ff, use_multi_head=True)\n",
    "    multi_head_output = multi_head_model(x)\n",
    "    print(\"多头注意力输出形状:\", multi_head_output.shape)\n",
    "    print(\"多头注意力输出:\", multi_head_output.detach().numpy())\n",
    "\n",
    "    # 单头注意力模型\n",
    "    set_seed(42)  # 再次设置相同种子\n",
    "    single_head_model = SimpleTransformer(vocab_size, d_model, num_heads, d_ff, use_multi_head=False)\n",
    "    single_head_output = single_head_model(x)\n",
    "    print(\"单头注意力输出形状:\", single_head_output.shape)\n",
    "    print(\"单头注意力输出:\", single_head_output.detach().numpy())\n",
    "\n",
    "    # 计算输出差异\n",
    "    diff = torch.abs(multi_head_output - single_head_output)\n",
    "    print(\"多头与单头输出差异（绝对值）:\", diff.detach().numpy())\n",
    "\n",
    "\n",
    "    # 验证权重是否相同\n",
    "    print(\"W_q 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_q.weight, single_head_model.transformer_layer.attention.W_q.weight))\n",
    "    print(\"W_k 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_k.weight, single_head_model.transformer_layer.attention.W_k.weight))\n",
    "    print(\"W_v 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_v.weight, single_head_model.transformer_layer.attention.W_v.weight))\n",
    "    print(\"W_o 权重是否相同:\", torch.equal(multi_head_model.transformer_layer.attention.W_o.weight, single_head_model.transformer_layer.attention.W_o.weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c89b4c6d6dd5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T07:34:12.730811Z",
     "start_time": "2025-04-04T07:34:09.048524Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "# 位置编码模块\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建位置编码表，形状为 [max_len, d_model]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 生成位置索引 [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算频率项，用于正弦和余弦函数\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 偶数维度用正弦函数\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 奇数维度用余弦函数\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 增加 batch 维度，形状变为 [1, max_len, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 注册为缓冲区，不参与梯度更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # 截取与输入序列长度匹配的位置编码，并加到输入上\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 多头自注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0  # 确保 d_model 可以被 num_heads 整除\n",
    "        self.d_model = d_model  # 模型维度\n",
    "        self.num_heads = num_heads  # 注意力头数\n",
    "        self.d_k = d_model // num_heads  # 每个头的维度\n",
    "\n",
    "        # 定义 Q、K、V 的线性变换层\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # 查询矩阵\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # 键矩阵\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # 值矩阵\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # 输出投影矩阵\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "\n",
    "        # 1. 计算 Q、K、V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # torch常用变换形状的函数\n",
    "        # view 将张量重新调整为指定的形状，要求新形状的元素总数与原张量一致\n",
    "        # reshape 类似 view，但更灵活，要求元素总数不变\n",
    "        # transpose 交换张量的两个维度，不改变数据顺序，只能交换两个维度\n",
    "        # unsqueeze 在指定位置增加一个维度（大小为 1），用于扩充张量维度\n",
    "        # squeeze 移除张量中大小为 1 的维度，用于缩小维度\n",
    "        # permute 重新排列张量的所有维度，比 transpose 更灵活。\n",
    "        # expand 将张量在大小为 1 的维度上扩展到指定大小，不复制数据（类似广播）\n",
    "        # 2. 将 Q、K、V 分成多头\n",
    "        # 变换形状为 [batch_size, seq_len, num_heads, d_k]，然后转置为 [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. 计算注意力分数\n",
    "        # scores = Q * K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        # 应用 softmax 得到注意力权重\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # 4. 用注意力权重加权 V\n",
    "        attn_output = torch.matmul(attn_weights, V)  # [batch_size, num_heads, seq_len, d_k]\n",
    "\n",
    "        # 5. 合并多头\n",
    "        # 转置回 [batch_size, seq_len, num_heads, d_k]，然后重塑为 [batch_size, seq_len, d_model]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        # 通过输出投影层\n",
    "        output = self.W_o(attn_output)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        return output\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        # 单头自注意力，Q、K、V 线性变换\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        # 1. 计算 Q、K、V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
    "        # 2. 计算注意力分数\n",
    "        # scores = Q * K^T / sqrt(d_model)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)  # [batch_size, seq_len, seq_len]\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "        # 3. 用注意力权重加权 V\n",
    "        attn_output = torch.matmul(attn_weights, V)  # [batch_size, seq_len, d_model]\n",
    "        # 4. 输出投影\n",
    "        output = self.W_o(attn_output)  # [batch_size, seq_len, d_model]\n",
    "        return output\n",
    "    \n",
    "# 前馈网络模块\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 第一层：扩展到 d_ff 维度\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        # 第二层：压缩回 d_model 维度\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # 通过第一层线性变换和激活函数\n",
    "        x = self.relu(self.linear1(x))  # [batch_size, seq_len, d_ff]\n",
    "        # 通过第二层线性变换\n",
    "        x = self.linear2(x)  # [batch_size, seq_len, d_model]\n",
    "        return x\n",
    "\n",
    "# 单层 Transformer 模块\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        # 多头自注意力\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        # 前馈网络\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # 1. 多头自注意力 + 残差连接 + 归一化\n",
    "        attn_output = self.attention(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.norm1(x + attn_output)  # 残差连接后归一化\n",
    "\n",
    "        # 2. 前馈网络 + 残差连接 + 归一化\n",
    "        ffn_output = self.ffn(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.norm2(x + ffn_output)  # 残差连接后归一化\n",
    "\n",
    "        return x\n",
    "\n",
    "# 完整 Transformer 模型\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, max_len=5000):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        # 嵌入层：将 token 索引转换为 d_model 维向量\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # 位置编码\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        # 单层 Transformer\n",
    "        self.transformer_layer = TransformerLayer(d_model, num_heads, d_ff)\n",
    "        # 输出层：将 d_model 维向量映射到词汇表大小\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]，输入是 token 索引\n",
    "        # 1. 嵌入层：将 token 索引转换为嵌入向量\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 2. 添加位置编码\n",
    "        embedded = self.pos_encoding(embedded)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 3. 通过 Transformer 层\n",
    "        transformed = self.transformer_layer(embedded)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 4. 输出层：映射到词汇表大小\n",
    "        output = self.output_layer(transformed)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a15f8c0e941d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T07:34:12.825978Z",
     "start_time": "2025-04-04T07:34:12.764959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 4])\n",
      "输出形状: torch.Size([2, 4, 1000])\n"
     ]
    }
   ],
   "source": [
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 参数设置\n",
    "    vocab_size = 1000  # 词汇表大小\n",
    "    d_model = 4       # 嵌入维度\n",
    "    num_heads = 2      # 注意力头数\n",
    "    d_ff = 16         # 前馈网络中间维度\n",
    "    batch_size = 2     # 批量大小\n",
    "    seq_len = 10       # 序列长度\n",
    "\n",
    "    # 创建模型\n",
    "    model = SimpleTransformer(vocab_size, d_model, num_heads, d_ff)\n",
    "\n",
    "    # 随机输入：模拟 token 索引\n",
    "    x = torch.tensor([[1,2,3,4], [4,3,2,1]])\n",
    "    print(\"输入形状:\", x.shape)\n",
    "\n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    print(\"输出形状:\", output.shape)  # [2, 10, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb796389c4049cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T07:36:04.875526Z",
     "start_time": "2025-04-04T07:36:04.863837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0900, 0.2447, 0.6652],\n",
      "         [0.0900, 0.2447, 0.6652],\n",
      "         [0.0900, 0.2447, 0.6652]]])\n",
      "tensor([[[0.0024, 0.0024, 0.0024],\n",
      "         [0.0473, 0.0473, 0.0473],\n",
      "         [0.9503, 0.9503, 0.9503]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
    "a1 = a\n",
    "print(a1.softmax(dim=-1))\n",
    "a2 = a\n",
    "print(a2.softmax(dim=-2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f2508ee7d10dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T07:58:15.119552Z",
     "start_time": "2025-04-04T07:58:15.108116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.4028234663852886e+38\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]]])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tensor([1, 0, 1], dtype=torch.float)\n",
    "mask_value = torch.finfo(a.dtype).min\n",
    "print(mask_value)\n",
    "print(a)\n",
    "a = torch.masked_fill(a, mask, mask_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df0cc867405a15a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:51:38.576264Z",
     "start_time": "2025-04-04T08:51:38.560545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.3535e-04, 9.9966e-01, 1.2660e-14],\n",
      "        [9.3576e-14, 1.0000e+00, 1.3710e-06],\n",
      "        [3.1391e-17, 1.0000e+00, 1.0262e-10]])\n",
      "tensor([[12.9990, 31.9896,  4.0017, 13.0044],\n",
      "        [13.0000, 32.0000,  4.0000, 13.0000],\n",
      "        [13.0000, 32.0000,  4.0000, 13.0000]])\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[1, 2, 3, 17], [4, 5, 6, 13], [7, 8, 9, 23]], dtype=torch.float)\n",
    "k = torch.tensor([[14, 3, 1, 9], [5, 7, 18, 7], [6, 22, 9, 3]], dtype=torch.float)\n",
    "v = torch.tensor([[10, 1, 9, 26], [13, 32, 4, 13], [7, 8, 3, 1]], dtype=torch.float)\n",
    "attention_score = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(4)\n",
    "attention_weights = attention_score.softmax(dim=-1)\n",
    "print(attention_score.softmax(dim=-1))\n",
    "output = torch.matmul(attention_weights, v)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24d23fc77ffe5bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:51:39.294696Z",
     "start_time": "2025-04-04T08:51:39.280049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3496.5000,  4991.0000,  1839.5000,  4751.5000],\n",
      "        [ 4411.0000,  6490.5000,  2233.0000,  5538.0000],\n",
      "        [ 6949.5000, 10076.0000,  3564.5000,  8900.5000]])\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[1, 2, 3, 17], [4, 5, 6, 13], [7, 8, 9, 23]], dtype=torch.float)\n",
    "k = torch.tensor([[14, 3, 1, 9], [5, 7, 18, 7], [6, 22, 9, 3]], dtype=torch.float)\n",
    "v = torch.tensor([[10, 1, 9, 26], [13, 32, 4, 13], [7, 8, 3, 1]], dtype=torch.float)\n",
    "feature_map = lambda x: torch.nn.functional.elu(x) + 1\n",
    "\n",
    "q = feature_map(q) / math.sqrt(4)  # 缩放\n",
    "k = feature_map(k)\n",
    "kv = torch.matmul(k.transpose(-1, -2), v)\n",
    "output = torch.matmul(q, kv)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb0421cd1226cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
