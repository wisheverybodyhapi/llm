{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于bert中文情感分类实战(根据下游任务微调模型)\n",
    "1. 我们只关注两个东西：输入和输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 下载数据集并加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/gfc/anaconda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9600/9600 [00:00<00:00, 454985.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1200/1200 [00:00<00:00, 157281.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1200/1200 [00:00<00:00, 153473.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集已保存到: /raid/gfc/llm/datasets/ChnSentiCorp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# 设置数据集目录\n",
    "dataset_dir = \"/raid/gfc/llm/datasets/ChnSentiCorp\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# 在线加载数据集\n",
    "data = load_dataset(\"lansinuote/ChnSentiCorp\")\n",
    "\n",
    "# 保存到dataset_dir目录\n",
    "data.save_to_disk(dataset_dir)\n",
    "print(f\"数据集已保存到: {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 制作Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, split, dataset_dir=\"/raid/gfc/llm/datasets/ChnSentiCorp\"):\n",
    "        self.dataset = load_from_disk(dataset_dir)\n",
    "        if split == \"train\":\n",
    "            self.dataset = self.dataset[\"train\"]\n",
    "        elif split == \"validation\":\n",
    "            self.dataset = self.dataset[\"validation\"]\n",
    "        elif split == \"test\":\n",
    "            self.dataset = self.dataset[\"test\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}. Must be one of ['train', 'validation', 'test']\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        label = self.dataset[idx][\"label\"]\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# 创建训练集\n",
    "train_dataset = Mydataset(split=\"train\")\n",
    "\n",
    "# for data in train_dataset:\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 根据下游任务修改模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import subprocess\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择空闲GPU: cuda:6\n",
      "显存占用: 3MB, GPU利用率: 0%\n",
      "当前使用的设备: cuda:6\n"
     ]
    }
   ],
   "source": [
    "# 1. 选择gpu\n",
    "def pick_free_gpu(start=7, end=0, memory_threshold=100):\n",
    "    \"\"\"\n",
    "    自动选择空闲的GPU\n",
    "    :param start: 起始GPU编号\n",
    "    :param end: 结束GPU编号\n",
    "    :param memory_threshold: 显存占用阈值（MB），低于此值认为GPU空闲\n",
    "    :return: torch.device对象\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 获取nvidia-smi输出，包含显存使用和GPU利用率\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used,utilization.gpu', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        # 解析输出\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            memory, util = map(int, line.split(', '))\n",
    "            gpu_info.append((memory, util))\n",
    "        \n",
    "        # 从start到end检查GPU\n",
    "        for i in range(start, end-1, -1):\n",
    "            if i < len(gpu_info):\n",
    "                memory_used, gpu_util = gpu_info[i]\n",
    "                # 判断条件：显存占用低于阈值且GPU利用率接近0\n",
    "                if memory_used < memory_threshold and gpu_util < 5:\n",
    "                    print(f\"选择空闲GPU: cuda:{i}\")\n",
    "                    print(f\"显存占用: {memory_used}MB, GPU利用率: {gpu_util}%\")\n",
    "                    return torch.device(f\"cuda:{i}\")\n",
    "        \n",
    "        print(\"没有检测到空闲GPU，使用CPU。\")\n",
    "        return torch.device(\"cpu\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"检测GPU时出错：{e}，使用CPU。\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# 使用示例\n",
    "device = pick_free_gpu(7, 0, memory_threshold=100)  # 设置显存阈值100MB\n",
    "print(f\"当前使用的设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt2 to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /raid/gfc/llm/models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(21128, 768, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "# 2. 加载预训练模型\n",
    "model_dir = \"/raid/gfc/llm/models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "pretrained = BertModel.from_pretrained(model_dir).to(device)\n",
    "\n",
    "# print(pretrained)\n",
    "# 打印输入\n",
    "print(pretrained.embeddings.word_embeddings)\n",
    "# 打印输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 定义下游任务模型\n",
    "# 4. 模型定义\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.pretrained = pretrained_model\n",
    "        # 使用多层分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(384, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 上游任务不参与训练\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.pretrained(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # 获取[CLS]标记的输出 (形状: [batch_size, 768])\n",
    "        cls_output = bert_output.last_hidden_state[:, 0]\n",
    "        \n",
    "        # 通过多层分类头\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 训练模型\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, AdamW\n",
    "\n",
    "Epoch = 50\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = Mydataset(split=\"train\")\n",
    "\n",
    "# 加载编码器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 自定义函数：对数据进行编码处理\n",
    "def collate_fn(batch):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "    # 批量编码文本\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    # 将labels转换为张量\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "        \"token_type_ids\": encoded[\"token_type_ids\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# 创建Dataloader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn, # 一般加载一批数据，编码一批数据\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始的bert模型在测试集上的精度为：0.49408783783783783\n",
      "训练100后的模型在测试集上的精度为：0.7204391891891891\n"
     ]
    }
   ],
   "source": [
    "# 测试模型精度\n",
    "# 加载测试集\n",
    "test_dataset = Mydataset(split=\"test\")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn, # 一般加载一批数据，编码一批数据\n",
    ")\n",
    "# 加载初始的bert模型在测试集上测试\n",
    "model = Model(pretrained_model=pretrained).to(device=device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "    print(f\"初始的bert模型在测试集上的精度为：{correct / total}\")\n",
    "\n",
    "# 加载训练好的模型在测试集上测试\n",
    "model_path = \"/raid/gfc/llm/params/bert_sentiment_classification/best_model.pt\"\n",
    "model = Model(pretrained_model=pretrained).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "    print(f\"训练{Epoch}后的模型在测试集上的精度为：{correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练（在py文件中训练）\n",
    "print(device)\n",
    "model = Model().to(device=device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(Epoch):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # 将数据放到device上\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # 前向传播得到输出\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss = loss_func(out, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            acc = (out == labels).sum().item() / len(labels)\n",
    "            print(epoch, i, loss.item(), acc)\n",
    "\n",
    "    # 保存模型参数\n",
    "    save_dir = \"/raid/gfc/llm/params/bert_sentiment_classification\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"{epoch}_bert.pt\"))\n",
    "    print(f\"{epoch} epoch 参数保存成功\") # 防止模型还没保存完，程序提前执行完毕\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
